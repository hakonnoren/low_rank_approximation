\documentclass[11pt,a4paper,english]{elsarticle}% 5p gir 2 kolonner pr side. 1p gir 1 kolonne pr side.
%\documentclass[11pt,a4paper,norsk]{article} % setter hvilken type dokument. Kan også være book eller report. I klammeparantes settes fontstørrelse, papirstørrelse og språk.

\usepackage[utf8]{inputenc} %- Løser problem med å skrive andre enn engelske bokstaver f.eks æ,ø,å.

\usepackage[T1]{fontenc} %- Støtter koding av forskjellige fonter.
\usepackage{lmodern}
\usepackage{textcomp} % Støtter bruk av forskjellige fonter som dollartegn, copyright, en kvart, en halv mm, se http://gcp.fcaglp.unlp.edu.ar/_media/integrantes:psantamaria:latex:textcomp.pdf

\usepackage{url} % Gjør internett- og e-mail adresser klikkbare i tex-dokumentet.

\usepackage{hyperref} % Gjør referansene i tex-dokumentet klikkbare, slik at du kommer til referansen i referanselista.

\usepackage[english]{babel} % Ordbok. Hvis man setter norsk i options til usepackage babel kan man bruke norske ord.

\usepackage{natbib}
\bibliographystyle{unsrtnat}

\urlstyle{sf} % Velger hvilken stil url-adresser skrives, f.eks sf

\usepackage{graphicx} % Brukes for å sette inn bilder eller figurer
\usepackage{amsmath} 				% Ekstra matematikkfunksjoner.
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{tikz-cd}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{changepage}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{bm}
\usepackage{physics}


\usepackage{tikz}
\usetikzlibrary[topaths]



\usepackage{siunitx}					% Må inkluderes for blant annet å få tilgang til kommandoen \SI (korrekte måltall med enheter)
	\sisetup{exponent-product = \cdot}      	% Prikk som multiplikasjonstegn (i steden for kryss).
 	\sisetup{output-decimal-marker  =  {,}} 	% Komma som desimalskilletegn (i steden for punktum).
 	\sisetup{separate-uncertainty = true}   	% Pluss-minus-form på usikkerhet (i steden for parentes). 

\usepackage{booktabs}                     		% For å få tilgang til finere linjer (til bruk i tabeller og slikt).

\usepackage[font=small,labelfont=bf]{caption}		% For justering av figurtekst og tabelltekst.


\journal{ }
\usepackage{etoolbox}
\makeatletter
\patchcmd{\ps@pprintTitle}
  {Preprint submitted to}
  {}
  {}{}
\makeatother
% Fjerner submitte dto 

% math stuff
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}

% my personal commands
\newcommand{\R}{\mathbb{R}}

% Denne setter navnet på abstract til Sammendrag
%\renewenvironment{abstract}{\global\setbox\absbox=\vbox\bgroup
%\hsize=\textwidth\def\baselinestretch{1}%
%\noindent\unskip\textbf{Sammendrag}
%\par\medskip\noindent\unskip\ignorespaces}
%{\egroup}

%\clearpage % Bruk denne kommandoen dersom du vil ha ny side etter det er satt plass til figuren.
% Disse kommandoene kan gjøre det enklere for LaTeX å plassere figurer og tabeller der du ønsker.
\setcounter{totalnumber}{5}
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.35}


% math stuff
\newtheorem{theorem}{Theorem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem*{observation}{Observation}
\newtheorem*{example}{Example}
\newtheorem*{remark}{Remark}

\graphicspath{{../}}

  \newcount\mycount

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{frontmatter}

\title{Numerical Linear Algebra}
\author[matematikk]{Håkon Noren}
\address[matematikk]{Department of Mathematical Science, Norwegian University of Science and Technology, N-7491 Trondheim, Norway.}
\end{frontmatter}

\section{Theory exercise 1}

%https://academic.oup.com/qjmath/article/11/1/50/1525786

\begin{theorem}
  Let $A \in \mathbb{R}^{m \times n}$ with the SVD decomposition $A = U\Sigma V^T$. Then the best approximation of $A$ in Frobenious norm is given by

  \begin{align*}
    A_k = \text{argmin}\|X - A\|_F
  \end{align*}

  where we restrict $X$ such that $\text{rank}(X) = k$. The best approximation is on the form

  \begin{align*}
    A_k = \sum_{j=1}^k \sigma_i\bm u_j \bm v_j^T
  \end{align*}
\end{theorem}


\begin{proof}
  As the Frobenious norm of a matrix $A$ is given by $\|A\|_F = (\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2)^{\frac{1}{2}}$ we have that

  \begin{align}
    \|A\|_F^2 &= \|U\Sigma V^T\|_F^2 \\
    &=\|\Sigma \|_F^2 \\
    &= \sum_{i=1}^n \sigma_i(A)^2
  \end{align}

  Furthermore by Von Neumanns trace inequality we have that

  \begin{align}
    |<A,B>| \leq \sum_{i=1}^n \sigma_i(A)\sigma_i(B)
  \end{align}

  Hence we can find the lower bound for the Frobenious norm

  \begin{align}
    \|X-A\|_F^2 &= <X-A,X-A>_F \\
    &= \|A\|_F^2 - 2<A,X>_F + \|X\|^2_F\\
    &\geq \sum_{i=1}^n \sigma_i(A)^2 -2\sum_{i=1}^n\sigma_i(A)\sigma_i(X) + \sum_{i=1}^n \sigma_i(X)^2 \\
    &= \sum_{i=1}^n(\sigma_i(A) - \sigma_i(X))^2
  \end{align}

  \noindent where it should be noted that this part is heavily inspired by [p. 58 Mirsky]\cite{Mirsky}.

  If $\sigma_1(A) \geq \sigma_2(A) \geq \cdots \geq \sigma_n(A)$ and $X$ is of rank $k$, then it is evident that we attain the lower bound when we have $X$ such that $\sigma_i(A) = \sigma_i(X)$ for $i = 1,\cdots,k$ and hence the lower bound is

  \begin{align}
    \|X-A\|^2_F \geq \sum_{k+1}^n \sigma_i(A)^2
  \end{align}

  It is then trivial to show that the lower bound is attained by $A_k$

  \begin{align*}
    \|A_k - A\|_F^2 &= \|U\Sigma_kV^T - U\Sigma V^T\|_F^2 \\
    &= \|\Sigma_k - \Sigma \|_F^2 \\
    &= \sum_{i = 1}^k (\bar \sigma_i - \sigma_i)^2 + \sum_{i = k+1}^n \sigma_i^2 \\
    &= \sum_{i = k+1}^n \sigma_i^2
  \end{align*}

  Hence $A_k$ is the best approximation of rank $k$ in the Frobenious norm.
\end{proof}


\section{Theory exercise 2}

\begin{theorem}
  Let $A \approx P_kB_kQ_k^T$ be the approximation of $A$ by Lanczos bidiagonalization, here $B$ is bidiagonal. Then the columns of $Q_k,P_k$ are orthonormal.
\end{theorem}

\begin{proof}
  We will in the proceeding proof refer to the notation used to describe the Lanczos bidiagonalization in \cite{Lanczos}. Lets first show that

  \begin{align*}
    <u_1,u_2> &= 0 \\
    <v_1,v_2> &= 0
  \end{align*}

  Where $\|u_1\|_2 = \|v_1\|_2 = 1$ is true by how the algorithm is initialized, from this we also can see that $\alpha_1 = v_1^TA^Tu_1 = u_1^TAv_1$. Hence we have


  \begin{align*}
    <u_1,u_2> &= \frac{1}{\beta_2}u_1^T(Av_1 - \alpha_1 u_1) \\
    &= \frac{1}{\beta_2} (\alpha_1 - \alpha_1u_1^Tu_1) = 0
    \\
    <v_1,v_2> &= \frac{1}{\alpha_2}v_1^T(A^Tu_2 - \beta_2 v_1) \\
    &= \frac{1}{\alpha_2}(v_1^TA^Tu_2 - v_1^TA^Tu_2v_1^Tv_1) = 0
  \end{align*}

  where we in the last line used that $\beta_2 = u_2^TAv_1 - \alpha_1u_2^Tu_1 = u_2^TAv_1$.

  We will now proceed with an induction hypothesis. Lets assume that, for some $l$ such that $1<l<m$ we have that

  \begin{align*}
    <u_i,u_{j}> &= \delta_{i,j} \\
    <v_i,v_{j}> &= \delta_{i,j} \; \; \text{for } \; 1 \leq i,j \leq l
  \end{align*}

  we then prove that from this assumption, orthogonality holds in general, for $l < i,j \leq < k$. From the assumption we see the following (for $1 \leq i,j \leq l$)

  \begin{align*}
    \alpha_i &= v_i^TA^Tu_i - \beta_i v_i^Tv_{i-1} = v_i^TA^Tu_i \\
  \end{align*}

  We have
  
  \begin{align*}
    <u_i,u_{j+1}> &= \frac{1}{\beta_{j+1}}u_i^T(Av_j - \alpha_j u_j) \\
    &\stackrel{(i = j = l)}{=} \frac{1}{\beta_{l+1}}(u_l^TAv_l - \alpha_l u_l^Tu_l)= \frac{1}{\beta_{l+1}}(\alpha_l - \alpha_l) = 0 \\
    \\
    <v_i,v_{j+1}> &= \frac{1}{\alpha_{j+1}}v_i^T(A^Tu_{j+1} - \beta_{j+1} v_j) \\
    &= \frac{1}{\alpha_{j+1}}(v_i^TA^Tu_{j+1} - u_{j+1}^TAv_jv_i^Tv_j - \alpha_ju_{j+1}^Tu_jv_i^Tv_j)\\
    &\stackrel{(i = j = l)}{=}\frac{1}{\alpha_{l+1}}(v_l^TA^Tu_{l+1} - u_{l+1}^TAv_l - \alpha_lu_{l+1}^Tu_l) \\
    &= \frac{\alpha_l}{\alpha_{l+1}}u_{l+1}^Tu_l = 0
  \end{align*}

  \noindent hence we have shown that the vectors $u_{l+1},v_{l+1}$ is orthogonal to its preceeding $u_i,v_i, i \leq l$. They are also normalized 

\begin{align*}
  <v_i,v_{j+1}> &= \frac{1}{\alpha_{j+1}}(v_i^TA^Tu_{j+1} - u_{j+1}^TAv_jv_i^Tv_j - \alpha_ju_{j+1}^Tu_jv_i^Tv_j)\\
  &\stackrel{(i = j+1 = l+1)}{=} \frac{1}{\alpha_{l+1}}(v_{l+1}^TA^Tu_{l+1} - u_{l+1}^TAv_lv_{l+1}^Tv_l - \alpha_lu_{l+1}^Tu_lv_{l+1}^Tv_l)\\
  &=\frac{1}{\alpha_{l+1}}(v_{l+1}^TA^Tu_{l+1}) =  \frac{1}{\alpha_{l+1}}(v_{l+1}^T(\alpha_{l+1}v_{l+1} + \beta_{l+1}v_l)) = \frac{\alpha_{l+1}}{\alpha_{l+1}} = 1\\
  \\
  <u_i,u_{j+1}> &= \frac{1}{\beta_{j+1}}(\alpha_iv_i^Tv_j + \beta_iv_{i-1}^Tv_j - \alpha_ju_i^Tu_j )\\
  &\stackrel{(i = j+1 = l+1)}{=} \frac{1}{\beta_{l+1}}(\alpha_{l+1}v_{l+1}^Tv_l + \beta_{l+1}v_{l}^Tv_l - \alpha_lu_{l+1}^Tu_l )\\
  &= \frac{\beta_{l+1}}{\beta_{l+1}} = 1
\end{align*}
%    (i = j+1 = l+1) \implies &= \frac{1}{\beta_i}(\alpha_iv_i^Tv_{i-1} + \beta_iv_{i-1}^Tv_{i-1} - \alpha_{i-1}u_i^Tu_{i-1} ) = \frac{\beta_i}{\beta_i} = 1 \\
%\end{align*}

%    (i \neq j+1, i \neq j) \implies &= \frac{1}{\beta_{j+1}}(\alpha_iv_i^Tv_j + \beta_iv_{i-1}^Tv_j - \alpha_ju_i^Tu_j ) = 0
%  \end{align*}

To conclude, we have shown that the assumption is indeed true for $l = 1$. And when assuming orthogonality for $u_i,v_i, i\leq l$ orthogonality follows for $u_{l+1},v_{l+1}$. Hence, by induction, $P_k := [u_1,\cdots,u_k], Q_k := [v_1,\cdots,v_k]$ have orthonormal columns. 
\end{proof}

\section{Theory exercise 3}

\begin{theorem}
  Let $U \in \mathbb{R}^{m \times k}, \; \text{st } U^TU = I$ and $B \in \mathbb{R}^{m\times m}, \; \text{st } B^T = -B$. Then for
\begin{align*}
  \bar U = \text{cay}(B)U
\end{align*}

\noindent we have that $\bar U^T \bar U = I$. 
\end{theorem}

\begin{proof}
  In general, if $F,G$ are orthonormal, then $D = FG$ is othonormal as we have

  \begin{align*}
    D^TD &= G^TF^TFG \\
    &= G^TG = I
  \end{align*}

  Hence it is sufficient to show that $\text{cay}(B)$ is orthogonal. We have

  \begin{align*}
    \text{cay}(B)^T\text{cay}(B) &=  (I+\frac{1}{2}B)^T(I-\frac{1}{2}B)^{-T}(I-\frac{1}{2}B)^{-1}(I+\frac{1}{2}B)\\
    &= (I-\frac{1}{2}B)(I+\frac{1}{2}B)^{-1}(I-\frac{1}{2}B)^{-1}(I+\frac{1}{2}B)
  \end{align*}

  Furthermore we can show that the terms of the Cayley transform commutes

  \begin{align*}
    (I-\frac{1}{2}B)^{-1}(I+\frac{1}{2}B) &= (I-\frac{1}{2}B)^{-1}((I-\frac{1}{2}B)(-1) + 2I)\\
    &= 2(I-\frac{1}{2}B)^{-1} \\
    &= (I+\frac{1}{2}B)(I-\frac{1}{2}B)^{-1}
  \end{align*}

  which gives us 

  \begin{align*}
    \text{cay}(B)^T\text{cay}(B) &=  (I+\frac{1}{2}B)^{-1}(I-\frac{1}{2}B)(I-\frac{1}{2}B)^{-1}(I+\frac{1}{2}B)\\
    &= I
  \end{align*}

Hence $\text{cay}(B)U$ is orthonormal.

\end{proof}

\section{Theory exercise 4}

\begin{theorem}
  Let $U,U^{\perp}$ be orthonormal matrices and $W$ be skew symmetric, ie. that $W^T = -W$. Then for $U,U^{\perp},W$ having suitable sizes we have

  \begin{align*}
    \text{cay}\bigg([U,U^{\perp}]W[U,U^{\perp}]^T\bigg) &= [U,U^{\perp}]\text{cay}(W)[U,U^{\perp}]^T. 
  \end{align*}
\end{theorem}

\begin{proof}
  Analytic functions such as the cayley transformation have power series expansions for a suitable choice of coefficients $\alpha$, on the form

  \begin{align*}
    \text{cay}(B) = \sum_{i=0}^{\infty} \alpha_i B^i
  \end{align*}

  for $B = [U,U^{\perp}]W[U,U^{\perp}]^T$ we see that the orthogonal terms collapse when taking powers

  \begin{align*}
    B^i &= [U,U^{\perp}]W\underbrace{[U,U^{\perp}]^T[U,U^{\perp}]}_{= I}W[U,U^{\perp}]^T\cdots[U,U^{\perp}]W\underbrace{[U,U^{\perp}]^T[U,U^{\perp}]}_{= I}W[U,U^{\perp}]^T \\
    &= [U,U^{\perp}]B^{i-1}\underbrace{[U,U^{\perp}]^T[U,U^{\perp}]}_{= I}B[U,U^{\perp}]^T \\
    &= [U,U^{\perp}]B^i[U,U^{\perp}]^T 
  \end{align*}

\noindent hence 

\begin{align*}
  \text{cay}\bigg([U,U^{\perp}]W[U,U^{\perp}]^T\bigg) &= [U,U^{\perp}]\bigg(\sum_{i=0}^{\infty} \alpha_i W^i\bigg)[U,U^{\perp}]^T \\
  &=  [U,U^{\perp}]\text{cay}(W)[U,U^{\perp}]^T
\end{align*}

\end{proof}

\begin{remark}
  For $U^{\perp}R_{2,2} = F_U$ being the QR-transformation of $F_U$, orthogonality is indeed the case for $U^{\perp}$. Furthermore it is trivial to see that when 

  \begin{align*}
    W = \begin{bmatrix}
      0&-R^T_{2,2} \\
      R_{2,2}&0
    \end{bmatrix}
  \end{align*}

  $W$ is skew symmetric.
\end{remark}


\bibliography{ref}

\end{document}


