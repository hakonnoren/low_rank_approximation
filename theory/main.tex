\documentclass[11pt,a4paper,english]{elsarticle}% 5p gir 2 kolonner pr side. 1p gir 1 kolonne pr side.
%\documentclass[11pt,a4paper,norsk]{article} % setter hvilken type dokument. Kan også være book eller report. I klammeparantes settes fontstørrelse, papirstørrelse og språk.

\usepackage[utf8]{inputenc} %- Løser problem med å skrive andre enn engelske bokstaver f.eks æ,ø,å.

\usepackage[T1]{fontenc} %- Støtter koding av forskjellige fonter.
\usepackage{lmodern}
\usepackage{textcomp} % Støtter bruk av forskjellige fonter som dollartegn, copyright, en kvart, en halv mm, se http://gcp.fcaglp.unlp.edu.ar/_media/integrantes:psantamaria:latex:textcomp.pdf

\usepackage{url} % Gjør internett- og e-mail adresser klikkbare i tex-dokumentet.

\usepackage{hyperref} % Gjør referansene i tex-dokumentet klikkbare, slik at du kommer til referansen i referanselista.

\usepackage[english]{babel} % Ordbok. Hvis man setter norsk i options til usepackage babel kan man bruke norske ord.

\usepackage{natbib}
\bibliographystyle{unsrtnat}

\urlstyle{sf} % Velger hvilken stil url-adresser skrives, f.eks sf

\usepackage{graphicx} % Brukes for å sette inn bilder eller figurer
\usepackage{amsmath} 				% Ekstra matematikkfunksjoner.
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{tikz-cd}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{changepage}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{bm}
\usepackage{physics}


\usepackage{tikz}
\usetikzlibrary[topaths]



\usepackage{siunitx}					% Må inkluderes for blant annet å få tilgang til kommandoen \SI (korrekte måltall med enheter)
	\sisetup{exponent-product = \cdot}      	% Prikk som multiplikasjonstegn (i steden for kryss).
 	\sisetup{output-decimal-marker  =  {,}} 	% Komma som desimalskilletegn (i steden for punktum).
 	\sisetup{separate-uncertainty = true}   	% Pluss-minus-form på usikkerhet (i steden for parentes). 

\usepackage{booktabs}                     		% For å få tilgang til finere linjer (til bruk i tabeller og slikt).

\usepackage[font=small,labelfont=bf]{caption}		% For justering av figurtekst og tabelltekst.


\journal{ }
\usepackage{etoolbox}
\makeatletter
\patchcmd{\ps@pprintTitle}
  {Preprint submitted to}
  {}
  {}{}
\makeatother
% Fjerner submitte dto 

% math stuff
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}

% my personal commands
\newcommand{\R}{\mathbb{R}}

% Denne setter navnet på abstract til Sammendrag
%\renewenvironment{abstract}{\global\setbox\absbox=\vbox\bgroup
%\hsize=\textwidth\def\baselinestretch{1}%
%\noindent\unskip\textbf{Sammendrag}
%\par\medskip\noindent\unskip\ignorespaces}
%{\egroup}

%\clearpage % Bruk denne kommandoen dersom du vil ha ny side etter det er satt plass til figuren.
% Disse kommandoene kan gjøre det enklere for LaTeX å plassere figurer og tabeller der du ønsker.
\setcounter{totalnumber}{5}
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.35}


% math stuff
\newtheorem{theorem}{Theorem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem*{observation}{Observation}
\newtheorem*{example}{Example}
\newtheorem*{remark}{Remark}

\graphicspath{{../}}

  \newcount\mycount

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{frontmatter}

\title{Low rank approximations: theorems with proofs}
\author[matematikk]{Håkon Noren}
\address[matematikk]{Department of Mathematical Science, Norwegian University of Science and Technology, N-7491 Trondheim, Norway.}
\end{frontmatter}

\section{Theory exercise 1}

%https://academic.oup.com/qjmath/article/11/1/50/1525786

\begin{theorem}
  Let $A \in \mathbb{R}^{m \times n}$ with the SVD decomposition $A = U\Sigma V^T$. Then the best approximation of $A$ in Frobenious norm is given by

  \begin{align*}
    A_k = \text{argmin}\|X - A\|_F
  \end{align*}

  where we restrict $X$ such that $\text{rank}(X) = k$. The best approximation is on the form

  \begin{align*}
    A_k = \sum_{j=1}^k \sigma_i\bm u_j \bm v_j^T
  \end{align*}
\end{theorem}


\begin{proof}
  As the Frobenious norm of a matrix $A$ is given by $\|A\|_F = (\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2)^{\frac{1}{2}}$ we have that

  \begin{align*}
    \|A\|_F^2 &= \|U\Sigma V^T\|_F^2 \\
    &=\|\Sigma \|_F^2 \\
    &= \sum_{i=1}^n \sigma_i(A)^2
  \end{align*}

  Furthermore by Von Neumanns trace inequality we have that

  \begin{align*}
    |<A,B>_F| \leq \sum_{i=1}^n \sigma_i(A)\sigma_i(B)
  \end{align*}

  Hence we can find the lower bound for the Frobenious norm

  \begin{align*}
    \|X-A\|_F^2 &= <X-A,X-A>_F \\
    &= \|A\|_F^2 - 2<A,X>_F + \|X\|^2_F\\
    &\geq \sum_{i=1}^n \sigma_i(A)^2 -2\sum_{i=1}^n\sigma_i(A)\sigma_i(X) + \sum_{i=1}^n \sigma_i(X)^2 \\
    &= \sum_{i=1}^n(\sigma_i(A) - \sigma_i(X))^2
  \end{align*}

  \noindent where it should be noted that this part is heavily inspired by [p. 58 Mirsky]\cite{Mirsky}.

  If $\sigma_1(A) \geq \sigma_2(A) \geq \cdots \geq \sigma_n(A)$ and $X$ is of rank $k$, then it is evident that we attain the lower bound when we have $X$ such that $\sigma_i(A) = \sigma_i(X)$ for $i = 1,\cdots,k$ and hence the lower bound is

  \begin{align*}
    \|X-A\|^2_F \geq \sum_{i=k+1}^n \sigma_i(A)^2
  \end{align*}

  It is then trivial to show that the lower bound is attained by $A_k$

  \begin{align*}
    \|A_k - A\|_F^2 &= \|U\Sigma_kV^T - U\Sigma V^T\|_F^2 \\
    &= \|\Sigma_k - \Sigma \|_F^2 \\
    &= \sum_{i = 1}^k (\bar \sigma_i - \sigma_i)^2 + \sum_{i = k+1}^n \sigma_i^2 \\
    &= \sum_{i = k+1}^n \sigma_i^2
  \end{align*}

  Hence $A_k$ is the best approximation of rank $k$ in the Frobenious norm.
\end{proof}


\section{Theory exercise 2}

\begin{theorem}
  Let $A \approx P_kB_kQ_k^T$ be the approximation of $A \in \mathbb{R}^{m \times n}, n \leq m$ by Lanczos bidiagonalization, here $B$ is bidiagonal. Then the columns of $Q_k,P_k$ are orthonormal.
\end{theorem}

\begin{proof}
  We will in the proceeding proof refer to the notation used to describe the Lanczos bidiagonalization in \cite{Lanczos}. Lets first show that

  \begin{align*}
    <u_1,u_2> &= 0 \\
    <v_1,v_2> &= 0
  \end{align*}

  \noindent Where $\|u_1\|_2 = \|v_1\|_2 = 1$ is true by how the algorithm is initialized, from this we also can see that $\alpha_1 = v_1^TA^Tu_1 = u_1^TAv_1$. Hence we have


  \begin{align*}
    <u_1,u_2> &= \frac{1}{\beta_2}u_1^T(Av_1 - \alpha_1 u_1) \\
    &= \frac{1}{\beta_2} (\alpha_1 - \alpha_1u_1^Tu_1) = 0
    \\
    <v_1,v_2> &= \frac{1}{\alpha_2}v_1^T(A^Tu_2 - \beta_2 v_1) \\
    &= \frac{1}{\alpha_2}(v_1^TA^Tu_2 - v_1^TA^Tu_2v_1^Tv_1) = 0
  \end{align*}

  \noindent Where we in the last line used that $\beta_2 = u_2^TAv_1 - \alpha_1u_2^Tu_1 = u_2^TAv_1$. It could also be shown that $<u_2,u_2> = <v_2,v_2> = 1$, but this follows directly from the algorithm, as we normalize the vectors using $\alpha_2,\beta_2$.

  We will now proceed with an induction hypothesis. Lets assume that, for some $l$ such that $1<l<k$ we have that

  \begin{align*}
    <u_i,u_{j}> &= \delta_{i,j} \\
    <v_i,v_{j}> &= \delta_{i,j} \; \; \text{for } \; 1 \leq i,j \leq l
  \end{align*}

  we then prove that from this assumption, orthogonality holds in general, for $l < i,j \leq k$. From the assumption we see the following (for $1 \leq i,j \leq l$)

  \begin{align*}
    \alpha_i &= v_i^TA^Tu_i - \beta_i v_i^Tv_{i-1} = v_i^TA^Tu_i \\
  \end{align*}

  We have
  
  \begin{align*}
    <u_i,u_{j+1}> &= \frac{1}{\beta_{j+1}}u_i^T(Av_j - \alpha_j u_j) \\
    &\stackrel{(i = j = l)}{=} \frac{1}{\beta_{l+1}}(u_l^TAv_l - \alpha_l u_l^Tu_l)= \frac{1}{\beta_{l+1}}(\alpha_l - \alpha_l) = 0 \\
    \\
    <v_i,v_{j+1}> &= \frac{1}{\alpha_{j+1}}v_i^T(A^Tu_{j+1} - \beta_{j+1} v_j) \\
    &= \frac{1}{\alpha_{j+1}}(v_i^TA^Tu_{j+1} - u_{j+1}^TAv_jv_i^Tv_j - \alpha_ju_{j+1}^Tu_jv_i^Tv_j)\\
    &\stackrel{(i = j = l)}{=}\frac{1}{\alpha_{l+1}}(v_l^TA^Tu_{l+1} - u_{l+1}^TAv_l - \alpha_lu_{l+1}^Tu_l) \\
    &= \frac{\alpha_l}{\alpha_{l+1}}u_{l+1}^Tu_l = 0
  \end{align*}

  \noindent hence we have shown that the vectors $u_{l+1},v_{l+1}$ is orthogonal to its preceeding $u_i,v_i, i \leq l$. They are also normalized 

\begin{align*}
  <v_i,v_{j+1}> &= \frac{1}{\alpha_{j+1}}(v_i^TA^Tu_{j+1} - u_{j+1}^TAv_jv_i^Tv_j - \alpha_ju_{j+1}^Tu_jv_i^Tv_j)\\
  &\stackrel{(i = j+1 = l+1)}{=} \frac{1}{\alpha_{l+1}}(v_{l+1}^TA^Tu_{l+1} - u_{l+1}^TAv_lv_{l+1}^Tv_l - \alpha_lu_{l+1}^Tu_lv_{l+1}^Tv_l)\\
  &=\frac{1}{\alpha_{l+1}}(v_{l+1}^TA^Tu_{l+1}) =  \frac{1}{\alpha_{l+1}}(v_{l+1}^T(\alpha_{l+1}v_{l+1} + \beta_{l+1}v_l)) = \frac{\alpha_{l+1}}{\alpha_{l+1}} = 1\\
  \\
  <u_i,u_{j+1}> &= \frac{1}{\beta_{j+1}}(\alpha_iv_i^Tv_j + \beta_iv_{i-1}^Tv_j - \alpha_ju_i^Tu_j )\\
  &\stackrel{(i = j+1 = l+1)}{=} \frac{1}{\beta_{l+1}}(\alpha_{l+1}v_{l+1}^Tv_l + \beta_{l+1}v_{l}^Tv_l - \alpha_lu_{l+1}^Tu_l )\\
  &= \frac{\beta_{l+1}}{\beta_{l+1}} = 1
\end{align*}

To conclude, we have shown that the assumption is indeed true for $l = 2$. And when assuming orthogonality for $u_i,v_i, i\leq l$ orthogonality follows for $u_{l+1},v_{l+1}$. Hence, by induction, $P_k := [u_1,\cdots,u_k], Q_k := [v_1,\cdots,v_k]$ have orthonormal columns. 
\end{proof}

\section{Theory exercise 3}

\begin{theorem}
  Let $U \in \mathbb{R}^{m \times k}$ with orthogonal columns and $B \in \mathbb{R}^{m\times m}, \; \text{st } B^T = -B$. Then for
\begin{align*}
  \bar U = \text{cay}(B)U
\end{align*}

\noindent $\bar U$ has orthogonal columns.
\end{theorem}

\begin{proof}
Lets first prove that $\text{cay}(B)$ is orthogonal

  \begin{align*}
    \text{cay}(B)^T\text{cay}(B) &=  (I+\frac{1}{2}B)^T(I-\frac{1}{2}B)^{-T}(I-\frac{1}{2}B)^{-1}(I+\frac{1}{2}B)\\
    &= (I-\frac{1}{2}B)(I+\frac{1}{2}B)^{-1}(I-\frac{1}{2}B)^{-1}(I+\frac{1}{2}B)
  \end{align*}

  Furthermore we can show that the terms of the Cayley transform commutes

  \begin{align*}
    (I-\frac{1}{2}B)^{-1}(I+\frac{1}{2}B) &= (I-\frac{1}{2}B)^{-1}((I-\frac{1}{2}B)(-1) + 2I)\\
    &= 2(I-\frac{1}{2}B)^{-1} \\
    &= (I+\frac{1}{2}B)(I-\frac{1}{2}B)^{-1}
  \end{align*}

  which gives us 

  \begin{align*}
    \text{cay}(B)^T\text{cay}(B) &=  (I+\frac{1}{2}B)^{-1}(I-\frac{1}{2}B)(I-\frac{1}{2}B)^{-1}(I+\frac{1}{2}B)\\
    &= I
  \end{align*}

Now taking

\begin{align*}
  \bar U^T \bar U &= U^T\text{cay}(B)^T\text{cay}(B)U\\
  &= U^TU
\end{align*}

As $U$ has orthonormal columns, then it follows that $\bar U$ has orthonormal columns.
\end{proof}

\section{Theory exercise 4}

\begin{theorem}
  Let $C = [F_U,-U], D = [U,F_U], C,D \in \mathbb{R}^{m \times 2k}$, and $F_U = U^{\perp}R_{2,2}$ be the QR-factorization. Then the Cayley transformation of $CD^T$ could be expressed as

  \begin{align*}
    \text{cay}(CD^T) &= I + [U,U^{\perp}]\begin{bmatrix}
      0&-R^T_{2,2}\\
      R_{2,2}&0
    \end{bmatrix}
    \biggl(I-\frac{1}{2}
    \begin{bmatrix}
      0&-R^T_{2,2}\\
      R_{2,2}&0
    \end{bmatrix}
    \biggl)^{-1}[U,U^{\perp}]^T.
  \end{align*}
\end{theorem}

\begin{proof}
  Analytic functions such as the cayley transformation have power series expansions for a suitable choice of coefficients $\alpha$, on the form

  \begin{align*}
    \text{cay}(B) = \sum_{i=0}^{\infty} \alpha_i B^i
  \end{align*}

  And for $B = CD^T$ it is possible to show that, if we can rewrite the power series as

  \begin{align*}
    \phi(B) &= \alpha_0 I + C \sum_{i=1}^{\infty}\alpha_i (D^TC)^{i-1}D^T
  \end{align*}

  \noindent then we can write the Cayley transform as

  \begin{align*}
    \text{cay}(CD^T) &= I + C(I+\frac{1}{2}D^TC)^{-1}D^T.
  \end{align*}

With the given QR-decomposition of $F_U$ we can write $CD^T$ on the form

  \begin{align*}
    CD^T &= [U,U^{\perp}]
    \begin{bmatrix}
      0&-R^T_{2,2}\\
      R_{2,2}&0
    \end{bmatrix}
    [U,U^{\perp}]^T
  \end{align*}


  Let now $W =  [U,U^{\perp}] \in \mathbb{R}^{m \times 2k}$ and $R \in \mathbb{R}^{2k \times 2k}$ be the above matrix with the upper diagonal QR-terms $R_{2,2}$. We can then show that the middle terms collapse when taking powers of $B$, i.e. that

  \begin{align*}
    B^i &= W\underbrace{RW^TWR}_{= R^2}W^T \cdots W\underbrace{RW^TWR}_{= R^2}W^T\\
    &= WR^{i-2}\underbrace{RW^TWR}_{= R^2}W^T\\
    &= WR^iW^T\\
  \end{align*}

  this is a fact, as we have

  \begin{align*}
    WRW^TWRW^T &= WR
    \begin{bmatrix}
    I&U^TU^{\perp}\\
    U^{\perp}U^T &I
    \end{bmatrix}
    RW^T\\
    &= W
    \begin{bmatrix}
    -R_{2,2}^TR_{2,2}&0\\
    0 &-R_{2,2}^TR_{2,2}
    \end{bmatrix}
    W^T\\
    &= WR^2W^T.
  \end{align*}

This enables us to construct a power series in a similar fashion as above

\begin{align*}
    \phi(B) &= \alpha_0 I + W \sum_{i=1}^{\infty}\alpha_i (R)^{i}W^T \\
     &= \alpha_0 I + W (\phi(R) - I)W^T \\
     &= \alpha_0 I + W R(I-\frac{1}{2}R)W^T \\
\end{align*}

\noindent which yields the expression we set out to prove

\begin{align*}
    \text{cay}(CD^T) &= I + [U,U^{\perp}]\begin{bmatrix}
      0&-R^T_{2,2}\\
      R_{2,2}&0
    \end{bmatrix}
    \biggl(I+\frac{1}{2}
    \begin{bmatrix}
      0&-R^T_{2,2}\\
      R_{2,2}&0
    \end{bmatrix}
    \biggl)^{-1}[U,U^{\perp}]^T.
  \end{align*}

\end{proof}

\bibliography{ref}

\end{document}


